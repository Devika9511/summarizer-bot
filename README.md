ğŸ“Œ Telegram YouTube Summarizer & Q&A Bot
Eywa SDE Intern Assignment
ğŸš€ Project Overview

This project is a Telegram-based AI assistant that helps users quickly understand long YouTube videos and interact with their content intelligently.

The bot can:

ğŸ”— Accept a YouTube link

ğŸ“œ Extract the video transcript

ğŸ§¾ Generate a structured summary

â“ Answer contextual follow-up questions

ğŸŒ Support multilingual input (English + Hindi)

The system is designed to behave like a lightweight AI research assistant for YouTube content.

ğŸ¯ Objective

The goal is to build a Telegram bot that:

ğŸ”— Accepts a YouTube link

ğŸ“¥ Fetches the video transcript

ğŸ§  Generates a structured summary

ğŸ’¬ Allows contextual Q&A

ğŸŒ Supports English and at least one Indian language

ğŸš« Ensures grounded responses (no hallucinations)

ğŸ§  Core Features Implemented
1ï¸âƒ£ Structured Summary Generation

When a user sends a YouTube link, the bot generates:

ğŸ¥ Video Title

ğŸ“Œ 5 Key Points

â± Important Timestamps

ğŸ§  Core Takeaway

âœ” The summary format is strictly enforced to avoid long paragraph dumps and improve clarity.

2ï¸âƒ£ Contextual Q&A (Grounded Responses)

Users can ask multiple follow-up questions about the video.

The system ensures:

ğŸ“„ Answers are generated strictly using transcript content

ğŸš« No external knowledge is used

âš  If the information is not present in the transcript, the bot responds:

"This topic is not covered in the video."

This ensures zero hallucination behavior.

3ï¸âƒ£ Multi-language Support

The bot supports:

ğŸ‡¬ğŸ‡§ English (default)

ğŸ‡®ğŸ‡³ Hindi input

Implementation Approach

If a user asks a question in Hindi:

ğŸ”„ Hindi question is translated into English

ğŸ§  Transcript-based reasoning is performed

ğŸ’¬ Stable response is generated

ğŸ“¢ Hindi fallback responses are used when appropriate

This translation-layer architecture enables multilingual support even on low-resource systems.

ğŸ— Architecture Design
ğŸ“‚ Project Structure
Summariser/
â”‚
â”œâ”€â”€ bot.py           â†’ Telegram bot logic and session handling
â”œâ”€â”€ transcript.py    â†’ Transcript extraction using yt-dlp
â”œâ”€â”€ summarizer.py    â†’ LLM interaction and structured prompts
â”œâ”€â”€ config.py        â†’ Token and model configuration
â””â”€â”€ README.md
ğŸ”„ System Flow

1ï¸âƒ£ User sends a YouTube link
2ï¸âƒ£ Transcript is extracted using yt-dlp
3ï¸âƒ£ Transcript is trimmed for memory efficiency
4ï¸âƒ£ Structured summary is generated using Ollama
5ï¸âƒ£ User asks follow-up questions
6ï¸âƒ£ Answers are generated using transcript context only

âš™ï¸ Model & Environment
ğŸ§  LLM Runtime

ğŸ–¥ Ollama (Local execution)

ğŸ¤– Model Used

phi3 (2.2GB model)

ğŸ’» System Constraints

RAM: 4GB

GPU: Not available

OS: Windows

âš  Due to RAM limitations:

âŒ Large models like Mistral or LLaMA 3 were not used

âœ‚ Transcript length is trimmed for safe inference

ğŸŒ¡ Temperature reduced for deterministic output

These decisions ensure system stability and reliability.

ğŸ” Transcript Retrieval Strategy

Transcript extraction is handled using:

yt-dlp --write-auto-sub

The system handles:

âŒ Invalid YouTube URLs

âš  Missing transcripts

ğŸ“‚ Empty subtitle files

ğŸ“œ Long transcripts

âš™ Command execution errors

Temporary subtitle files are removed after processing.

ğŸ§  Context Management

Each Telegram user session is stored independently using:

user_sessions[user_id]

This ensures:

ğŸ‘¥ Multiple users can interact simultaneously

ğŸ’¬ Conversations remain contextual

ğŸ”’ Sessions do not interfere with each other

ğŸ¯ Q&A Grounding Strategy

The model is explicitly instructed to:

ğŸ“„ Use only transcript information

ğŸš« Avoid outside knowledge

âŒ Refuse unrelated questions

âœ Provide concise answers (3â€“5 lines)

This ensures reliable and grounded responses.

ğŸŒ Multi-language Design Decision

Instead of forcing multilingual reasoning inside the model, a translation-layer approach was used:

Hindi â†’ English â†’ Grounded reasoning â†’ Response
Advantages:

âœ” Better reliability with small models

âœ” Reduced hallucination risk

âœ” Works well on low-resource systems

âœ” Meets assignment flexibility requirements

ğŸ›  Setup Instructions
1ï¸âƒ£ Install Ollama

Download from:
https://ollama.com

Pull the model:

ollama pull phi3
2ï¸âƒ£ Install Python Dependencies

Inside the project folder run:

pip install python-telegram-bot requests yt-dlp
3ï¸âƒ£ Configure Telegram Bot

Create a bot using BotFather on Telegram

Copy the bot token

Add it inside:

config.py
4ï¸âƒ£ Run the Bot
python bot.py
âš– Design Trade-offs

ğŸ§  Lightweight model used due to 4GB RAM constraint

âœ‚ Transcript trimmed to reduce memory usage

ğŸŒ¡ Temperature reduced for structured output

ğŸŒ Translation layer used instead of full multilingual generation

âš¡ No embedding pipeline used to keep the system lightweight

ğŸ¥ Demo Flow

â–¶ Start the bot

ğŸ”— Send a YouTube link

ğŸ“„ Receive structured summary

â“ Ask contextual follow-up questions

ğŸ‡®ğŸ‡³ Ask a question in Hindi

âš  Observe fallback when topic is not covered

ğŸ“Œ Conclusion

This system demonstrates:

ğŸ¤– Practical AI assistant development

ğŸ§  Structured prompt engineering

ğŸ“„ Grounded LLM usage

ğŸ‘¥ Multi-user session management

ğŸ’» Hardware-aware architecture design

The solution is designed to be stable, scalable, and production-conscious even under constrained hardware resources.
